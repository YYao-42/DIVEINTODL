{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "13_3BoundingBox.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YYao-42/DIVEINTODL/blob/main/Chapter-13-Computer-Vision/13_3BoundingBox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZRhSvbKN1aG"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0InYXFWWN1aJ"
      },
      "source": [
        "!pip install d2l==0.17.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "jGzMEBuHN1aK"
      },
      "source": [
        "# Object Detection and Bounding Boxes\n",
        ":label:`sec_bbox`\n",
        "\n",
        "\n",
        "In earlier sections (e.g., :numref:`sec_alexnet`--:numref:`sec_googlenet`),\n",
        "we introduced various models for image classification.\n",
        "In image classification tasks,\n",
        "we assume that there is only *one*\n",
        "major object\n",
        "in the image and we only focus on how to \n",
        "recognize its category.\n",
        "However, there are often *multiple* objects\n",
        "in the image of interest.\n",
        "We not only want to know their categories, but also their specific positions in the image.\n",
        "In computer vision, we refer to such tasks as *object detection* (or *object recognition*).\n",
        "\n",
        "Object detection has been\n",
        "widely applied in many fields.\n",
        "For example, self-driving needs to plan \n",
        "traveling routes\n",
        "by detecting the positions\n",
        "of vehicles, pedestrians, roads, and obstacles in the captured video images.\n",
        "Besides,\n",
        "robots may use this technique\n",
        "to detect and localize objects of interest\n",
        "throughout its navigation of an environment.\n",
        "Moreover,\n",
        "security systems\n",
        "may need to detect abnormal objects, such as intruders or bombs.\n",
        "\n",
        "In the next few sections, we will introduce \n",
        "several deep learning methods for object detection.\n",
        "We will begin with an introduction\n",
        "to *positions* (or *locations*) of objects.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 2,
        "tab": [
          "pytorch"
        ],
        "id": "qJMH2ySNN1aL"
      },
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "from d2l import torch as d2l\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 4,
        "id": "gfwZ6IwBN1aL"
      },
      "source": [
        "We will load the sample image to be used in this section. We can see that there is a dog on the left side of the image and a cat on the right.\n",
        "They are the two major objects in this image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 6,
        "tab": [
          "pytorch"
        ],
        "id": "urFMiqEON1aL"
      },
      "source": [
        "d2l.set_figsize()\n",
        "img = d2l.plt.imread('../img/catdog.jpg')\n",
        "d2l.plt.imshow(img);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 7,
        "id": "3IPibcvuN1aN"
      },
      "source": [
        "## Bounding Boxes\n",
        "\n",
        "\n",
        "In object detection,\n",
        "we usually use a *bounding box* to describe the spatial location of an object.\n",
        "The bounding box is rectangular, which is determined by the $x$ and $y$ coordinates of the upper-left corner of the rectangle and the such coordinates of the lower-right corner. \n",
        "Another commonly used bounding box representation is the $(x, y)$-axis\n",
        "coordinates of the bounding box center, and the width and height of the box.\n",
        "\n",
        "[**Here we define functions to convert between**] these (**two\n",
        "representations**):\n",
        "`box_corner_to_center` converts from the two-corner\n",
        "representation to the center-width-height presentation,\n",
        "and `box_center_to_corner` vice versa.\n",
        "The input argument `boxes` should be a two-dimensional tensor of\n",
        "shape ($n$, 4), where $n$ is the number of bounding boxes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 8,
        "tab": [
          "pytorch"
        ],
        "id": "H_eMr_WON1aN"
      },
      "source": [
        "def box_corner_to_center(boxes):\n",
        "    \"\"\"Convert from (upper-left, lower-right) to (center, width, height).\"\"\"\n",
        "    x1, y1, x2, y2 = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
        "    cx = (x1 + x2) / 2\n",
        "    cy = (y1 + y2) / 2\n",
        "    w = x2 - x1\n",
        "    h = y2 - y1\n",
        "    boxes = torch.stack((cx, cy, w, h), axis=-1)\n",
        "    return boxes\n",
        "\n",
        "def box_center_to_corner(boxes):\n",
        "    \"\"\"Convert from (center, width, height) to (upper-left, lower-right).\"\"\"\n",
        "    cx, cy, w, h = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3]\n",
        "    x1 = cx - 0.5 * w\n",
        "    y1 = cy - 0.5 * h\n",
        "    x2 = cx + 0.5 * w\n",
        "    y2 = cy + 0.5 * h\n",
        "    boxes = torch.stack((x1, y1, x2, y2), axis=-1)\n",
        "    return boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 9,
        "id": "UDlaw9aAN1aO"
      },
      "source": [
        "We will [**define the bounding boxes of the dog and the cat in the image**] based\n",
        "on the coordinate information.\n",
        "The origin of the coordinates in the image\n",
        "is the upper-left corner of the image, and to the right and down are the\n",
        "positive directions of the $x$ and $y$ axes, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 10,
        "tab": [
          "pytorch"
        ],
        "id": "gEE99e-jN1aP"
      },
      "source": [
        "# Here `bbox` is the abbreviation for bounding box\n",
        "dog_bbox, cat_bbox = [60.0, 45.0, 378.0, 516.0], [400.0, 112.0, 655.0, 493.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 11,
        "id": "X0CMBtv3N1aP"
      },
      "source": [
        "We can verify the correctness of the two\n",
        "bounding box conversion functions by converting twice.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 12,
        "tab": [
          "pytorch"
        ],
        "id": "OP5-gU7sN1aQ"
      },
      "source": [
        "boxes = torch.tensor((dog_bbox, cat_bbox))\n",
        "box_center_to_corner(box_corner_to_center(boxes)) == boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 13,
        "id": "AUSQU0OkN1aQ"
      },
      "source": [
        "Let us [**draw the bounding boxes in the image**] to check if they are accurate.\n",
        "Before drawing, we will define a helper function `bbox_to_rect`. It represents the bounding box in the bounding box format of the  `matplotlib` package.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 14,
        "tab": [
          "pytorch"
        ],
        "id": "rEZ5Mm70N1aQ"
      },
      "source": [
        "def bbox_to_rect(bbox, color):\n",
        "    \"\"\"Convert bounding box to matplotlib format.\"\"\"\n",
        "    # Convert the bounding box (upper-left x, upper-left y, lower-right x,\n",
        "    # lower-right y) format to the matplotlib format: ((upper-left x,\n",
        "    # upper-left y), width, height)\n",
        "    return d2l.plt.Rectangle(xy=(bbox[0], bbox[1]), width=bbox[2] - bbox[0],\n",
        "                             height=bbox[3] - bbox[1], fill=False,\n",
        "                             edgecolor=color, linewidth=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 15,
        "id": "eMLwPgsSN1aR"
      },
      "source": [
        "After adding the bounding boxes on the image,\n",
        "we can see that the main outline of the two objects are basically inside the two boxes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 16,
        "tab": [
          "pytorch"
        ],
        "id": "GTqTfF7fN1aR"
      },
      "source": [
        "fig = d2l.plt.imshow(img)\n",
        "fig.axes.add_patch(bbox_to_rect(dog_bbox, 'blue'))\n",
        "fig.axes.add_patch(bbox_to_rect(cat_bbox, 'red'));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 17,
        "id": "kQf7a663N1aR"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* Object detection not only recognizes all the objects of interest in the image, but also their positions. The position is generally represented by a rectangular bounding box.\n",
        "* We can convert between two commonly used bounding box representations.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Find another image and try to label a bounding box that contains the object. Compare labeling bounding boxes and categories: which usually takes longer?\n",
        "1. Why is the innermost dimension of the input argument `boxes` of `box_corner_to_center` and `box_center_to_corner` always 4?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 19,
        "tab": [
          "pytorch"
        ],
        "id": "-fTlMn_fN1aR"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/1527)\n"
      ]
    }
  ]
}